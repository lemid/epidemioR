# Estudos de sensibilidade de populações de patógenos

## Introdução

Estudos de CE50 (concentração efetiva) são muito utilizados, pois permite obter uma curva dose-resposta, que relaciona diferentes concentrações do produto utilizado no controle da doença, sendo então possível estimar uma concentração a ser utilizada, ou mesmo monitorar a mudança da sensibilidade do patógeno. Se a CE50 de um fungicida, ao longo do tempo, apresentar alterações para valores superiores aos inicialmente estabelecidos, para obter controle de um determinado patógeno, poderá indicar redução na sensibilidade do patógeno ao controle com aquele fungicida (@Reis, 2007). 

Geralmente se utiliza ao menos 5 concentrações, variando para mais gradientes do mesmo produto, para possibilitar a visualização de níveis de controle e assim uma melhor estimativa do valor de CE50. O valor de controle sempre é calculado em relação a testemunha, ou seja, um tratamento sem o uso do fungicida. Os parâmetros geralmente utilizados nas avaliações de CE50, visam expressar o grau de toxicidade de uma substância, que representa a concentração de um composto químico onde 50% de seu efeito máximo é observado (@Oga, 2008). 

O uso de modelos não lineares aos dados de CE50 é constantemente aplicada em estudos de eficiência de moléculas. Devido a grande quantidade de modelos não lineares, a tarefa de escolher o modelo mais adequado para utilizar não é fácil e demanda de conhecimento e interpretação do resultado correto. Porém, para que haja uma boa interpretação dos dados e escolha do modelo, é necessário quantificar algum parâmetro que forneça subsidio que permita diferenciar se o modelo tem um ajuste “adequado” ou “inadequado”. A medida mais comumente utilizada na literatura cientifica é o coeficiente de determinação R2. Para analises lineares, essa medida é intuitiva, pois fornecem rápida interpretação de quanto a variação dos dados se explica pelo modelo (@Spiess2010). 

O valor de R2 varia de 0 a 1, sendo que quanto mais próximo de 1, maior será o ajuste dos dados ao modelo. Há algum tempo foi descrita como inadequada a medida do R2 para regressões não lineares (@SA, FM 1987; @Kvlseth1985). Porém no meio cientifico, pesquisadores e revisores continuam a utilizar e/ou exigir que essa medida seja disponibilizada em artigos que utilizam modelos não lineares. Essa abordagem possivelmente ocorre, pois pesquisadores geralmente aplicam métodos estatísticos sem possuírem um detalhado conhecimento estatístico (@Spiess2010). 

A maioria dos softwares estatísticos disponíveis (R, Sisvar, Systat, Statistica, Prism, Origin, Matlab, SPSS, SAS, Prism) calculam os valores de R2 para ajustes não lineares, o que possivelmente contribua involuntariamente para o seu frequente uso. Isso também pode ocorrer com os usuários do software TableCurve2 D (Systat, EUA), que possibilita ajustar centenas de modelos não lineares a um determinado conjunto de dados e depois classificá-los por meio de R2 (@Spiess2010).

Diversos esforços foram realizados para desenvolver medidas do tipo R2 para os modelos de regressão não linear, porem é descrito que essa abordagem é inadequada e se comparada com os valores de AICc, BIC, variância residual e qui-quadrado, pois a medida de R2 raramente é afetada mais do que na terceira ou quarta casa decimal (@ColinCameron1997). Simulações de Monte Carlo mostraram que o AIC, o AICc ou o BIC têm um desempenho muito melhor nesse sentido. 

Os valores de R2 ainda estão  sendo utilizados em publicações no intuito de demonstrar o desempenho ou validar modelos, e isso pode ser o resultado de autores ou revisores não estarem cientes dessa falácia. Por isso autores e revisores devem compreender que solicitar valores de R2 para validar modelos não lineares não é o ideal, e isto deve ser substituído ou complementado pelos valores AIC / AICc / BIC que são valores mais aplicáveis para validar modelos (@Spiess2010).   

O critério de informação dos valores de Akaike (AIC; Akaike, 1973) é um método popular para comparar a adequação de vários modelos, criando uma solução direta para o problema de avaliar vários modelos candidatos é selecionar o modelo que fornece a descrição mais precisa dos dados. No entanto, o processo de avaliação do modelo é complicado pelo fato de um modelo com muitos parâmetros livres ser mais flexível do que um modelo com apenas alguns parâmetros. Claramente, não é desejável considerar sempre o melhor modelo mais complexo, e geralmente é aceito que o melhor modelo é aquele que fornece uma descrição adequada dos dados.
enquanto utiliza um número mínimo de parâmetros (Myung, Forster e Browne, 2000; Myung e Pitt, 1997). Qualquer critério para a seleção de modelos precisa tratar dessa troca entre precisão descritiva e parcimônia.
Um dos métodos mais populares de comparação de vários modelos, levando em consideração tanto a precisão descritiva quanto a analogia, é o critério de informação de Akaike (AIC; ver, por exemplo, Akaike, 1973, 1974, 1978, 1979, 1983, 1987; Bozdogan, 1987; Burnham & Anderson, 2002; Parzen, Tanabe e Kitagawa, 1998; Sakamoto, Ishiguro e Kitagawa, 1986; Takane & Bozdogan, 1987).1 A AIC é um método muito geral aplicado em uma ampla gama de situações relevantes para a psicologia cognitiva. A AIC também é aplicada na análise fatorial (por exemplo, Akaike, 1987), regressão (por exemplo, Burnham & Anderson, 2002). 

Comparação entre AIC e BIC
Apesar do amplo uso da AIC, alguns acreditam que ela é muito liberal e tende a selecionar modelos excessivamente complexos (por exemplo, Kass & Raftery, 1995). Foi apontado que a AIC negligencia a variabilidade amostral dos parâmetros estimados. Quando os valores de probabilidade para esses parâmetros não estão altamente concentrados em torno de seu valor máximo, isso pode levar a avaliações excessivamente otimistas (para uma ilustração, consulte Aitchison & Dunsmore, 1975, pp. 227–234). Além disso, o AIC não é consistente. Ou seja, conforme o número de observações n aumenta muito, a probabilidade de a AIC recuperar um verdadeiro modelo de baixa dimensão não se aproxima da unidade (por exemplo, Bozdogan, 1987, p. 357). Um critério popular de seleção de modelo alternativo é o critério de informação bayesiano ou BIC (por exemplo, Burnham & Anderson, 2002; Hastie, Tibshirani e Friedman, 2001; Kass & Raftery, 1995; Schwarz, 1978; Wasserman, 2000). O BIC (Schwarz, 1978) para o modelo i é definido como:

BICi = -2logLi + Vi logn, (5)

onde n é o número de observações que entram no cálculo da probabilidade. O BIC é uma aproximação assintótica para uma análise de seleção de modelo bayesiano (BMS), na qual se integra no espaço de parâmetros. 

Ao contrário do AIC, o BIC é consistente como nao e leva em consideração a incerteza dos parâmetros. Uma comparação entre BIC (Equação 5) e AIC (Equação 1) mostra que o termo de penalidade do BIC é maior que o termo de penalidade do AIC. Embora as equações de AIC e BIC pareçam muito semelhantes, elas se originam de estruturas bastante diferentes. O BIC pressupõe que o modelo de geração real esteja no conjunto de modelos candidatos, e mede o grau de crença de que um determinado modelo é o verdadeiro modelo de geração de dados. A AIC não supõe que nenhum dos modelos candidatos seja necessariamente verdadeiro, mas calcula para cada modelo a discrepância de Kullback-Leibler, que é uma medida da distância entre a densidade de probabilidade gerada pelo modelo e a realidade. Uma comparação formal em termos de desempenho entre a AIC e a BIC é muito difícil, principalmente porque a AIC e a BIC abordam questões diferentes. A maioria das simulações que mostram que o BIC tem um desempenho melhor que o AIC pressupõe que o modelo verdadeiro esteja no conjunto candidato e que seja de dimensão relativamente baixa. Por outro lado, a maioria das simulações que favorecem o AIC em detrimento do BIC é realmente dimensional e, portanto, o modelo real não está no conjunto candidato.

Os pesos de Akaike são fáceis de calcular a partir dos valores brutos da AIC e fornecem uma interpretação direta, pois as probabilidades de cada modelo ser o melhor modelo no sentido da AIC. O uso de pesos Akaike oferece ao leitor uma maior compreensão dos méritos relativos dos modelos concorrentes. Além disso, os pesos de Akaike quantificam as conclusões com base nas análises da AIC, especificando a quantidade de confiança estatística para o modelo com o menor valor da AIC.4 Dadas essas vantagens consideráveis, acreditamos que, em muitas circunstâncias, é muito útil supor complementar os resultados padrão da análise de comparação do modelo AIC com a apresentação dos pesos Akaike.

Desta forma, neste capitulo será abordado o uso de modelos não lineares para avaliar ensaios de CE50 para populações de ferrugem asiática da soja e nematoides, e diferentes compostos serão utilizados visando o controle destes patógenos. Será abordado o uso do software R para calcular as CE50 e plotar os respectivos gráficos. Vamos detalhar a utilização do pacote DRC e NLME para ambos exemplos biológicos. Abordaremos o melhor ajuste de modelo analisando os valores de AIC, BIC e Loglike para os dados de cada CE50, comparando-os com a linearização das concentrações com o LOG10 e discutindo modelos linear e não-lineares e suas respectivas interpretações. 


## Ensaios de sensibilidade de *Phakopsora pachyrhizi*

A soja, *Glycine max* (L.) Merr, é uma das commodities mais importantes no Brasil e a nível mundial, pois é fonte de óleo e proteína para alimentação humana e animal (@conab). A produtividade dessa cultura pode ser afetada por doenças, principalmente a ferrugem asiática da soja – FAS, causada pelo fungo *Phakopsora pachyrhizi* Syd. & P. Syd por Hans e Paul Sydow. O agente causal da FAS pertence ao reino Fungi, classe de Basidiomycetos, ordem Uredinales, família Phakopsoraceae, gênero Phakopsora, espécie pachyrhizi (@GOELLNER2010). Trata-se de um patógeno biotrófico, o qual não se reproduz em meio axênico (@HARTMAN, 2011). 

No Brasil a doença foi constatada em 2001, na safra 2001/2002 no estado do Paraná, na região oeste, fronteira do Paraguai (Foz do Iguaçu à Guaíra), e avançou rapidamente, se espalhando para outros estados (@YORINORI, 2005). Atualmente, a FAS está presente em todas regiões produtoras da soja no mundo, porém vem sendo um grave problema principalmente na América do Sul, devido a ocorrência de todas a vértices do triângulo da doença, com grande expressão do ambiente (@MURITHI, 2015). É a doença mais grave para a cultura, pois o custo (perda em grãos e gasto com controle) é estimado em dois bilhões de dólares por safra (@CAF, 2018). 

Trata-se de um patógeno muito agressivo, pois a FAS é uma doença policíclica, assim inúmeros ciclos de infecção podem ocorrer em um único ciclo do hospedeiro (@GODOY, 2016), garantindo assim a disseminação dos urediniósporos dentro da mesma área de cultivo e áreas adjacentes (@BROWN, 2002). Além disso, possui forma ágil dispersão dos esporos pelo vento, curto período latente e longo período infeccioso, sendo que as condições ambientais favoráveis para o desenvolvimento da soja também são favoráveis para o patógeno, e a sua constante evolução permite a perda de sensibilidade a fungicidas e a quebra de genes de resistência de plantas de soja melhoradas geneticamente (@GODOY, 2016).

As principais estratégias de manejo da doença para reduzir o risco de danos à cultura são: a utilização de cultivares de ciclo precoce, realização de semeadura no início da época recomendada, uso de cultivares com resistência genética, eliminação de plantas tigueras, respeitar o vazio sanitário de no mínimo 60 dias, sendo recomendado 90 dias pelo consórcio Anti-Ferrugem, e ainda realizar desde o início do desenvolvimento da cultura constante monitoramento da lavoura, e assim que surgir os primeiros sintomas e/ou ainda de forma preventiva utilizar fungicidas, este sendo o mais utilizado para o controle desta doença (@GODOY, 2016)

Os principais fungicidas utilizados no manejo da FAS, são os inibidores de desmetilação (IDM’s), inibidores de quinona externa (IQe’s), inibidores da succinato desidrogenase (ISDH’s) e o tradicional mancozebe. Falhas no controle da FAS tem sido observada nos últimos anos em diferentes regiões do Brasil, devido a mutações do fungo, que conferem resistência aos fungicidas [@godoy2019]. 

Foi relatado mutações em isolados de *P. pachyrhizi* nos genes CYP51 [@schmitz2013], CYT-b [@klosowski2015] e SDH-c [@simoes2017]. Genótipos com resistência nos genes CYP51 e CYT-b são mais comumente encontrados [@klosowski2016] porém recentemente foi descrito a ocorrência de múltipla resistência para estes três genes no mesmo isolado [@muller]. A constante evolução do patógeno e a pressão de seleção imposta pelo uso constante de fungicidas, permitiu que o fungo apresente alta frequência da sua população resistente para as moléculas disponíveis. Portanto, estudos que monitorem a sensibilidade do fungo aos fungicidas são cada vez mais importantes.
Para a P. pachyrhizi os estudos de CE50 - concentração efetiva para controlar 50% do patógeno são realizados estudos em folha destacada (ex vivo) em placas de Petri, pois permite avaliar a sensibilidade do fungo em condições controladas de laboratório, sem demandar de grande estrutura. 

A metodologia descrita por (@Scherb, 2006) é difundida mundialmente pela FRAC (Comitê de Ação a Resistencia a Fungicidas). Essa metodologia consiste de ensaio ex vivo, com folhas destacadas, tratadas com os diferentes concentrações de fungicida e acondicionadas em placa de Petri com meio Agar-agua e incubadas em em BOD a 22°C com fotoperiodo de 12h por cerca de 15 a 20 dias até o surgimento e estabilização dos sintomas do patógeno nas folhas, que então serão avaliados e/ou quantificados com o uso de escala diagramática que possibilita avaliar a severidade da doença. Para tal avaliação, se utiliza a escala diagramática de (@GODOY2006) (FIGURA 1) que contempla seis níveis de intensidade de dano em folhas, variando de 0,6 a 78,5% de área foliar afetada pela ferrugem. 

```{r, image1, echo = FALSE, fig1 = '(ref:fig1)'}
knitr::include_graphics("./Alexandre/fig1.jpg")
```

Neste trabalho será abordado análises estatísticas com o uso do software R, para avaliar a eficiência de controle de fungicidas, para populações do fungo *P. pachyrhizi* oriundas de diferentes regiões produtoras do Brasil. Será analisado diferentes ajustes de modelos matemáticos para dados de CE50, com o intuito de propor novas abordagens para este tipo de dados, devido grande variação encontrada na literatura nas análises estatísticas para dados de CE50, o que dificulta na interpretação, e pode gerar valores para CE50 com grande variação para um mesmo conjunto de dados, o que pode sub ou superestimar a sensibilidade avaliada.  Espera-se também facilitar o uso do R no meio cientifico com a descrição detalhada dos pacotes utilizados para as analises estatísticas. 

Na situação hipotética deste capitulo, em que se deseja estudar o efeito de moléculas no controle da FAS, utilizou-se 8 concentrações de fungicida para obter os valores de CE50 para 6 isolados populacionais de P. pachyrhizi oriundas de diferentes regiões. Neste caso será abordado análise estatística com o software estatístico R, comparando e plotando os valores de CE50 para as populações e verificando em qual modelo matemático se obtém os melhores ajustes dos dados.

```{r, image2, echo = FALSE, fig2 = '(ref:fig2)'}
knitr::include_graphics("./Alexandre/fig2.jpg")
```


### Estimando a EC50 com o pacote NLME

O exemplo a seguir utiliza um script desenvolvido para estimar a EC50 de 6 diferentes populações de *P. pachyrhizi*. Cada um dos isolados foi testado com 8 concentrações (0; 0,05; 0,5; 1; 5; 15; 50; 100 mg L^-1) de um fungicida. O experimento foi realizado em esquema fatorial 6X8 em delineamento inteiramente casualizado com 4 repetições.
O script demanda da instalação dos pacotes nlme, tidyverse e ggplot2. Os procedimentos a seguir apontam os comando necessários para o procedimento. A instalação dos pacotes será necessário ser realiza somente uma vez.

```{r, message=TRUE}
### Carregar os pacotes
library(nlme)
library(tidyverse)
library(ggplot2)
library(RCurl)
```

Os dados foram organizados em uma planilha em formato .csv. A primeira coluna denominada "trat"" continha os tratamentos avaliados no fator 1. Nesse caso, 6 populações do patógeno. Na segunda coluna, denominada como "conc", estavam descritas as concentrações testadas. Na terceira coluna foi expressa a variável resposta denominada "inib", correspondendo a inibição em relação a média da testemunha. A inibição foi calculada utilizando a equação :

Os dados importados para o R serão salvos em uma dataframe denominado "tb"

```{r}
### Importação dos dados em .csv salvos no github
tb <- read.csv2("https://raw.githubusercontent.com/lemid/epidemioR/master/alexandre/p_pachyrhizi.csv", sep=";", dec=",", header = T)

### Explorar os dados dentro de "tb"
head(tb)
str(tb)
summary(tb)

### Plotar as curvas
ggplot(tb,
       aes(x = conc, y = inib)) +
  facet_wrap(facets = ~trat) +
  geom_point()
```

Nesse exemplo, iremos testar o ajuste dos dados aos modelos não lineares de curvas sigmoidais de Gompertz e Weibull. Ambos os modelos possuem 3 parametros de ajuste: A, B e C. O parametro A corresponde a assintota máxima limite da função quando x vai para o infinito. O parametro C é relacionado com a taxa no ponto de inflexão. B é um parâmetro de forma relacionado com a posição do ponto de inflexão. 
O modelo de gompertz é expresso pela equação:
f(x) = A * (exp(-exp(-C * (X - B))))

A equação que descreve o modelo de Weibull é expressa como:
f(x) = A * exp(-exp(-C * (log(X) - B)))

Ambos os modelos serão abordados como exemplo de uso do pacote nlme para estimar a EC50. Entretanto, o script apresentará somente o passo-a-passo para o modelo de Gompertz, poderá ser substituido de acordo com as sugestões apontadas durante o script.

```{r}
### Equação de Gompertz
gompertz <- inib ~ A * exp(-exp( -C * (conc - B)))

### Equação de Weibull
weibull <- inib ~ A * (exp(-exp( -C * (log(conc) - B))))
```

O ajuste os dados nesse modelo utilizando o pacote nlme demanda de estimativas iniciais para os parametros, que aqui chamamos de "chutes iniciais". Esses "chutes" correspondem a assintota máxima da curva (A), o ponto de inflexão da curva (B) e a taxa (C). Para facilitar os chutes iniciais, utilizaremos a função SSgompertz e nls do pacote nlme. Essas funções combinadas permitem estimar os valores dos parametros com um nível de tolerância, permitindo a inserção na função gnls.

```{r}
### Estima curvas para os dados
fit <- nls(inib ~ SSgompertz(conc, A, B, C), data=tb)

### Extrai os coeficientes das curvas
coef(fit)

### Expões o número de níveis que geram curvas
levels(tb$trat)
```

Os valores dos parametros estimados pela função nls serão utilizados na função gnls. Os valores de cada parametro serão repetidos o número de níveis - 1. Em nosso exemplo, os valores aproximados dos paramêtros foram de 94 para A, 0.9 para C e 3 para B. O número de níveis -1 é igual a 5. Se o modelo de Weibull for testado, deve-se substituir o "model" pela equação descrita anteriormente.

```{r}
### Primeiro ajuste
m1 <- gnls(model = gompertz,
           data = tb,
           params = A + C + B ~ trat,
           start = c(79, rep(0, times = 5),
                     0.9, rep(0, times = 5),
                     2, rep(0, times = 5)))

### Extrair os coeficientes para confirmação
coef(m1)
```

A utilização dos "chutes" permitiu o primeiro ajuste dos dados no modelo. Entretanto, quando extraimos os valores dos parametros dos modelos para cada uma das curvas, observamos que os valores dos parametros não condizem com os valores que as curvas expressam. Ou seja, a assintota máxima é o ponto máximo da curva onde há a estabilização. Quando plotamos as curvas, observamos que a assintoma máxima é de aproximadamente 90%. Como nosso ajuste apontou valores do parametro A muito distantes de 90%, devemos realizar um novo ajuste. Nessa situação, utilizaremos novamente os chutes iniciais que utilizamos no primeiro ajuste. Nesse segundo ajuste, não iremos descontar um nível, e usaremos 6.

```{r}
### Segundo ajuste
m2 <- gnls(model = gompertz,
           data = tb,
           params = A + C + B ~ 0 + trat,
           start = c(rep(95, times = 6),
                     rep(0.3, times = 6),
                     rep(2.7, times = 6)))

### Extrair os coeficientes para confirmação
coef(m2)
```

O segundo ajuste apontou valores condizentes para os parâmetros. Para confirmar que atingimos um ajuste adequado, subtituiremos os valores de chutes iniciais pelos valores dos parametros para cada curva. Para facilitar a extração dos valores dos parametros, é possivel utilizar os comando abaixo para criar uma planilha .csv.

```{r}
### Extrair os coeficientes do ajuste
coef <- coef(m2) %>%
  as.data.frame() %>%
  rownames_to_column("trat")
```

Em posse dos dados para os parametros, podemos adicionar os valores correspondentes aos parametros.
```{r}
### Ajuste de confirmação
m3 <- gnls(model = gompertz,
           data = tb,
           params = A + C + B ~ 0 + trat,
           start = c(c(95.61736708,
                       71.08692254,
                       91.24407557,
                       98.93633749,
                       69.34402308,
                       82.42489619),
                     c(0.279264323,
                       0.044141583,
                       0.0765223,
                       0.044048882,
                       0.106096686,
                       0.040753042),
                     c(2.736348255,
                       13.71446866,
                       9.390132928,
                       18.67955398,
                       7.092374429,
                       23.32987732)))

### Extrair os coeficientes para confirmação
coef(m3)

# Tabela com as estimativas dos parâmetros para cada curva
summary(m3)$tTable
```

Com o ajuste de confirmação, podemos observar que as novas estimativas para os parametros são muito proximas daquelas que obtivemos com o segunde ajuste. Com os dados ajustados ao modelo, podemos extrair os valores da curva do modelo e os resíduos dos dados em relação a curva do modelo. Os valores da curva do modelo e os resíduos serão acomodados no dataframe com os dados iniciais.

```{r}
### Resíduos e curva do modelo
tb$res <- residuals(m3)
tb$fit <- fitted(m3)

### Dataframe com os dados iniciais 
head(tb)

### Gráfico com os resíduos contra o tempo
ggplot(tb,
       aes(x = conc, y = res)) +
  facet_wrap(facets = ~trat) +
  geom_point()

### Gráfico com os resíduos contra os valores ajustados
ggplot(tb,
       aes(x = fit, y = res)) +
  facet_wrap(facets = ~trat) +
  geom_point()

### Gráfico de normalidade dos resíduos.
ggplot(tb,
       aes(sample = res)) +
  facet_wrap(facets = ~trat) +
  geom_qq()

```

```{r}
### Grid para a predição com bandas
conc_seq <- seq(0, 100, by = 0.5)
grid <- with(tb,
             expand.grid(conc = conc_seq,
                         trat = levels(trat)))

grid$fit <- predict(m3, newdata = grid)
head(grid)

### Para retornar a matriz gradiente para o vetor `conc`
partials <- deriv(gompertz[-2],
                  namevec = c("A", "C", "B"),
                  function.arg = function(conc, A, C, B) {
                    NULL
                  })

### Matriz com os parâmetros estimados po curva
params <- matrix(coef(m3),
                 ncol = 3,
                 dimnames = list(levels(tb$trat),
                                 c("A", "C", "B")))

### Lista de matrizes gradiente
grad <- sapply(levels(tb$trat),
               simplify = FALSE,
               FUN = function(trat) {
                 u <- do.call(partials,
                              args = c(list(conc = conc_seq),
                                       as.list(params[trat, ])))
                 u <- attr(u, "gradient")
                 colnames(u) <- sprintf("%s.trat%s",
                                        colnames(u),
                                        trat)
                 return(u)
               })

### Criação da matriz bloco diagonal e atribui os tratamentos
X <- as.matrix(Matrix::bdiag(grad))
colnames(X) <- c(sapply(grad, colnames))

### Troca colunas de lugar para corresponder com `vcov()`.
X <- X[, colnames(vcov(m3))]

### Calcula o erro padrão do valor predito e o intervalo de confiança
qtl <- qt(0.975, df = nrow(tb) - length(coef(m3)))
grid$se <- sqrt(diag(X %*% vcov(m3) %*% t(X)))
grid <- grid %>%
  mutate(lwr = fit - qtl * se,
         upr = fit + qtl * se)
```

A estimativa de EC50
```{r}
params <- params %>%
  as.data.frame() %>%
  rownames_to_column("trat")

head(params)

tb_params <- params%>%as.data.frame()%>%
  mutate(EC50 =  B - ((log(-log(50/A)))/C))

head(tb_params)
```

O coeficiente de determinação como já descrito anteriormente nesse capítulo, não é um inidicativo de ajuste confiável. Entretanto, sabemos que diversos periódicos e revisores insistem nesse dado. Nesse caso, podemos obter os valores de R^2 utilizando os comando abaixo:

```{r}
### Cálcula o R^2 e cria o data frame tb_R^2
tb_r2 <- tb %>%
  group_by(trat) %>%
  summarise(R2 = cor(inib, fit)^2)

### Insere o valor do R^2 na tabela de paramêtros
tb_params <- tb_params %>% inner_join(tb_r2)

### Confirmação da criação da coluna R2
head(tb_params)
```

A expressão gráfica dos resultados é uma ferramenta impressindivel para muitos casos, podendo ser a melhor maneira de expressar os resultados.
```{r}
### Legenda contendo o valor de R^2 e EC50 com duas casas decimais 
fmt <- "R2: %0.2f \n EC50: %0.2f"

### Plotar os dados em 6 gráficos
grafico <- ggplot(tb, aes(x = conc, y = inib)) +
  facet_wrap(facets = ~trat, nrow = 2) 

### Insere a curva do modelo
grafico <- grafico + geom_line(data = grid,
            mapping = aes(x = conc, y = fit))

### Insere o intervalo de confiança
grafico <- grafico + geom_ribbon(data = grid,
              inherit.aes = FALSE,
              mapping = aes(x = conc, ymin = lwr, ymax = upr),
              fill = "darkorange",
              alpha = 0.4) 

### Insere os pontos correspondentes aos dados
grafico <- grafico + geom_point(pch = 1)

### Insere a legenda
grafico <- grafico + geom_text(data = tb_params, 
                               mapping = aes(x = 100,
                                             y = 0,
                                             label = sprintf(fmt, R2, EC50)),
                               parse = FALSE,
                               size = 3,
                               hjust = 1,
                               vjust = 0)

### Insere os eixos e os títulos
grafico <- grafico + geom_hline(yintercept = 50,
                                lty = 3,
                                size = 0.5,
                                col = "darkorange") +
  scale_y_continuous(breaks = seq(0, 100, by = 20)) +
  scale_x_continuous(breaks = seq(0, 100, by = 20)) +
  labs(x = "Concentração",
       y = "Inibição (%)")

### Plota o gráfico
grafico
```

Os valores do parametros, R^2 e a estimativa do EC50 serão organizados no data frame tb_params. Esse data frame poderá ser exportado para um arquivo .csv para facilitar a manipulação.
```{r}
nlme_g3 <- tb_params
```


### Estimando a EC50 com o pacote DRC

Os dados de sensibilidade de *P. pachyrhizi* podem ser analisados pelo pacote DRC [@Ritzetal_2015]. Para tanto, são necessários alguns pacotes que precisam ser carregados antes de iniciar o script. O carregamento dos pacotes é realizado pelos comandos abaixo.

```{r}
library(drc)
library(ggplot2)
library(lmtest)
library(tidyverse)
```

Utilizaremos os mesmos dados utilizados no exemplo com o pacote nlme. O processo de importação dos dados é igual aos descrito anteriormente.

```{r}
### Importação dos dados em .csv salvos no github
tb <- read.csv2("https://raw.githubusercontent.com/lemid/epidemioR/master/alexandre/p_pachyrhizi.csv", sep=";", dec=",", header = T)

### Explorar os dados dentro de "tb"
head(tb)
str(tb)
summary(tb)

### Plotar as curvas
ggplot(tb,
       aes(x = conc, y = inib)) +
  facet_wrap(facets = ~trat) +
  geom_point()
```

O pacote drc não exige o carregamento de uma equação, pois o pacote já conta com as equações em suas funções. Para ajustar os dados à equação de Gompertz com três parâmetros (como utilizado no exemplo com o pacote nlme), utilizare-mos somente o comando G.3.

```{r}
### Ajustando ao modelo de Gompertz de três parâmetros
drc1.ll3 <- drm(inib ~ conc, trat, data = tb, fct = G.3())
```

Se houver interesse de verificar o ajuste dos dados aos demais modelos, podemos submeter o ajuste a função que irá comparar o primeiro ajuste com os demais modelos selecionados. Como exemplo, iremos comparar o modelo de Gompertz de três paramêtros com os modelos logistico (L.3), logistico natural (LN.3), log-logistico (LL.3) e Weibull (W2.3), todos com três parametros. 

```{r}
### Selecionando o melhor modelo global
mselect(drc1.ll3, list(L.3(), LN.3(), LL.3(), W2.3()))
```

Comparado ao pacote nlme, o pacote drc não demanda de chutes iniciais para estimar os valores dos parametros. A função drm permite estimar os valores dos parametros do modelo.

```{r}
### Montagem de um modelo com nomes atribuídos aos parâmetros
drc2.G.3 <- drm(inib ~ conc, trat, data = tb, 
                 fct = G.3(names = c("C", "A", "B")))
```

O modelo já conseguiu estimar a curva.Com os dados ajustados ao modelo, podemos extrair os valores da curva do modelo e os resíduos dos dados em relação a curva do modelo. Os valores da curva do modelo (fit) e os resíduos (res) serão acomodados no dataframe com os dados iniciais.

```{r}
### Adicionando a coluna da curva estimada
tb$fit <- fitted(drc2.G.3)

### Adicionando a coluna dos residuos
tb$res <- residuals(drc2.G.3)

# Confirmação da inclusão dos residuos e fit
head(tb)

# Resíduos contra o tempo.
ggplot(tb,
       aes(x = conc, y = res)) +
  facet_wrap(facets = ~trat) +
  geom_point()

# Resíduos contra os valores ajustados.
ggplot(tb,
       aes(x = fit, y = res)) +
  facet_wrap(facets = ~trat) +
  geom_point()

# Normalidade dos resíduos.
ggplot(tb,
       aes(sample = res)) +
  facet_wrap(facets = ~trat) +
  geom_qq()

```

Os valores dos parametros precisam ser organizados em uma tabela. Os comando abaixo criam uma tabela com os três parametros extraidos do modelo (A, B e C). Uma coluna será criada para acomodar o valor de R^2 para cada curva.
```{r}
### Cria a tabela com os parametros para cada curva
params <- matrix(coef(drc2.G.3),
                 ncol = 3,
                 dimnames = list(levels(tb$trat),
                                 c("C", "A", "B")))

### Adiciona a coluna tratamentos na tabela de parametros
params <- params %>%
  as.data.frame() %>%
  rownames_to_column("trat")

### Calcula o R2 das curvas
tb_r2 <- tb %>%
  group_by(trat) %>%
  summarise(R2 = cor(inib, fit)^2)

### Nomeia as colunas
names(tb_r2) <- c("trat", "R2")

### Adiociona a coluna de R2 na tabela com os valores dos parametros
tb_params <- tb_params %>% inner_join(tb_r2)

head(tb_params)

```

Outra facilidade do drc em relação ao nlme, é o calculo de EC50. O drc não demanda da derivada da equação do modelo para extrair o valor de EC50. A função ED extrai os valores da EC50 estimada para curva. Os valores do intervalo de confiança das estimativas também são calculados e expressos em uma tabela.

```{r}
### Estimativa de Ec50
EC50 <- ED(drc2.G.3, 50, interval = "delta")

EC50 <- EC50 %>%
  as.data.frame() %>%
  rownames_to_column("trat")

EC50 <- EC50 %>%  mutate(trat = as.numeric(trat))

EC50$trat <- levels(tb$trat)

names(EC50) <- c("trat", "EC50", "Error", "lwr", "upr")

tb_params <- tb_params %>% inner_join(EC50)

tb_params <- tb_params %>% 
  mutate_if(is.numeric, round, digits = 2)

tb_params

### Grid para a predição com bandas.
conc_seq <- seq(0, 100, by = 0.5)
grid <- with(tb,
             expand.grid(conc = conc_seq,
                         trat = levels(trat)))

grid <- grid %>%
  as.data.frame() %>%
  rownames_to_column("ponto")

fit <- predict(drc2.G.3, newdata = grid, interval = "confidence")

fit <- fit %>%
  as.data.frame() %>%
  rownames_to_column("ponto")

head(fit)

grid <- grid %>% inner_join(fit)

names(grid) <- c("ponto", "conc", "trat", "fit", "lwr", "upr")

head(grid)
```

```{r}
fmt <- "R2: %0.2f \n EC50: %0.2f"

### Gráfico com bandas de confiança.
grafico <- ggplot(tb, aes(x = conc, y = inib)) +
  facet_wrap(facets = ~trat, nrow = 4) +
  geom_line(data = grid,
            mapping = aes(x = conc, y = fit)) +
  geom_ribbon(data = grid,
              inherit.aes = FALSE,
              mapping = aes(x = conc, ymin = lwr, ymax = upr),
              fill = "darkorange",
              alpha = 0.4) 

grafico <- grafico + geom_point(pch = 1)

grafico <- grafico + geom_text(data = tb_params, 
                               mapping = aes(x = 100,
                                             y = 0,
                                             label = sprintf(fmt, R2, EC50)),
                               parse = FALSE,
                               size = 3,
                               hjust = 1,
                               vjust = 0)

grafico <- grafico + geom_hline(yintercept = 50,
                                lty = 3,
                                size = 0.5,
                                col = "darkorange") +
  scale_y_continuous(breaks = seq(0, 100, by = 20)) +
  scale_x_continuous(breaks = seq(0, 100, by = 20)) +
  labs(x = "Concentração",
       y = "Inibição (%)")

grafico
```


## Ensaios de sensibilidade de *Meloidogyne* spp.

Os nematoides são animais pertencentes ao filo Nematoda, que engloba cerca de 4000 espécies. São essencilamente aquáticos, podendo ser encontrados em todos os ecossistemas da terra. Um grupo desses animais são de extrema importância para os seres humanos, pois causam diversos prejuizos, parasitando plantas e animais. 
Na agricultura, os nematoides são extremamente importantes pois parasitam as raizes de plantas de interesse agrícola. Esse grupo de nematoides possuem em orgão destinado exclusivamente para se alimentar do protoplasto das células das plantas, o estilete. Esse orgão atua como uma agulha de uma seringa, que ao penetrar a célula vegetal, é capaz de sugar seu conteudo. Esse habito faz com que os nematoides sejam exclusivamente dependentes das plantas, toranando-os parasitas obrigatórios. Considerando a dependência nutricional dos nematoides fitoparasitas, podemos considera-los uma preucupação constante para a agricultura. Esses animais geram prejuízos anuais estimados em US$ 157 bilhões [@abad_etal_2008; @hassan_etal_2013; @singh_etal_2013] a US$ 358 bilhões [@abd_2014].

As espécies de nematoides que mais ocorrem no Brasil são *Heterodera glycines* Ichinohe, 1951, *Meloidogyne incognita* (Kofoid & White) Chitwood, 1949, *Meloidogyne javanica* (Treub, 1885) Chitwood, 1949, *Pratylenchus brachyurus* (Godfrey, 1929) Filipjev & S. Stekhoven, 1941 e *Rotylenchulus reniformis* Linford & Oliveira, 1940 [@dias_etal_2010; @franzener_etal_2005; @lobo_etal_2017; @mattos_etal_2016; @mazzetti_etal_2019; @miamoto_etal_2017; @miamoto_etal_2018]. 

Os nematoides do gênero *Meloidogyne* são endoparasitas e possuem a caracteristica marcante de causar galhas na raizes das plantas parasitadas. Essas galhas são causadas pela penetração do estilete dos juvenis nas células das raízes das plantas. Ao inserir o estilete, o nematoide inicia a alimentação e segrega substâncias que inibem a resposta da planta e induzem a formação do sítio de alimentação. Esse sítio de alimentação irá direcionar fotossimilados e nutrientes das células vizinhas para a célula que está sendo sugada pelo juvenil. Ao se alimentar, o juvenil se desenvolve e realiza as ecdises necessárias para seu desenvolvimento. Ao atingir o estágio adulto, a fêmea possuirá o corpo periforme (fom o formato de pêra) e produzirá uma massa de ovos que se acumula em uma matriz gelatinosa que em alguns casos poderá romper a parede da galha e ser exposta para fora das raízes da planta parasitada. Os ovos, cerca de 2000, irão iniciar um novo ciclo de parasitismo.

Os métodos de controle mais utilizados para o controle de nematoides no Brasil é o manejo integrado de pragas e o controle químico [@hassan_etal_2013]. O manejo integrado nem sempre é usado devido à complexidade da aplicação, e na opinião de alguns agricultores, os métodos alternativos podem não ser eficiente, tornando o controle químico com agrotóxicos a principal opção dos agricultores [@sharifzadeh_etal_2018]. Entretanto, a aplicação de agrotóxicos causa graves desequilíbrios ambientais [@carvalho_2017; @long_krupke_2016] e problemas de saúde nos seres humanos [@caldas_2016; @hendges_etal_2019]. Assim, métodos de controle menos agressivos a saúde e ao meio ambiente vem sendo estudados.

Os estudos de sensibilidade dos nematoides aos compostos e extratos é realizado por curvas de dose resposta. Assim, determinando o efeito de dose-resposta, é possivel estimar a concentração eficiente para obter o controle desejado. Esses estudos são realizados utilizando gradientes de concentrações de produtos, nos quais as fases de vida do nematoide são expostos ao tratamento por determinado periodo de tempo. Esses experimentos são comumente chamados de testes de eclosão, testes de motilidade e mortalidade, teste de penetração de juvenis e testes de Reprodução (Fator de reprodução).

Os testes de dose-resposta são realizados ao menos com 5 tratamentos, cada um deles com uma concentração diferente do produto. Geralmente são adicionados tratamentos testemunhas, no qual a concentração do produto testado é 0%. Cada concentração vai gerar uma resposta, afim de obter uma curva de resposta. Èssa curva deve formar um gradiente, onde as respostas apresentem diferentes níveis. Quando as concentrações não representam um gradiente de resposta, a estimativa da concentração efetiva pode ser subestima ou superestimada.

```{r, curvaexemplo, echo = FALSE}
### Importação dos dados em .csv salvos no github
tb <- read.csv2("https://raw.githubusercontent.com/lemid/epidemioR/master/alexandre/erro.csv", sep=";", dec=",", header = T)

library(drc)
erro.G.3 <- drm(inib ~ conc, trat, data = tb, 
                 fct = G.3(names = c("C", "A", "B")))
tb$fit <- fitted(erro.G.3)

### Plotar as curvas
library(ggplot2)
grafico <- ggplot(tb, aes(x = conc, y = inib)) +
  facet_wrap(facets = ~trat) +
  geom_point() + 
  scale_y_continuous(breaks = seq(0, 100, by = 10)) +
  scale_x_continuous(breaks = seq(0, 20, by = 5)) +
  labs(x = "Concentração",
       y = "Inibição (%)")
```

A curva de resposta corresponde a uma curva sigmoidal típica. Frequentemente os experimentos de expression(CE_50) apresentam somente parte da curva total, onde pode ocorrer ajuste em modelos lineares. A curva de dose resposta geralmente utilizada em estudos de CE50 para nematoides é baseada na curva de dose resposta baseada em um estudo de Seefeldt et al. (1995).

$$
y = f(x) = C + \frac{D - C}{1 + exp\left [ b \left ( log ( x \right ) - log\left ( CE_{50} \right ) \right )]}
$$

Onde: C é o limite inferior da curva, D é limite superior, b é o "Slup" e CE_50 é a concentração capaz de inibir em 50% a atividade dos nematoides.


### Análise de Dose-Resposta no R

Os dados obtidos pelas avaliações de um experimento preisam ser tabuladas em uma planilha. A planilha precisará ser organizada em colunas, no qual a primeira coluna corresponde ao tratamento. Na segunda coluna, devem ser dispostos as rapetições ou blocos. As colunas posteriores correspondem a variável resposta, das quais podem ser tantas quanto for necessário. A planilha poderá ser salvo em .txt, .csv ou .xlsc. Em nosso exemplo usaremos um arquivo em formato .csv. O arquivo poderá ser criado utilizando o comando de salvar como em csv (separado por ponto e vírgula). 

A importação dos dados da planilha será realizada usando os comandos abaixo:

A verificação do comportamento dos dados é importante para verificar a ocorrência de pontos fora da curva. Em nosso exemplo vamos plotar os dados em gráficos do estilo boxplot. Esse tipo de gráfico permitirá observar os dados, as médias e o desvio padrão correspondente a cada tratamento.
Os dados originais de mortalidade não serão utilizados para a estimativa de CE50. Entretanto, a verificação dos dados originais de mortalidade permitirão identificar pontos discordantes. 

Os dados corrigidos pela equação de SCHNEIDER-ORELLI, 1947 foram expressos na planilha na coluna corr. Após a correção pela equação, a mortalidade natural da testemunha é eliminada dos dados, deste modo, os valores da testemunha após a correção são valores próximos de 0 (zero).
